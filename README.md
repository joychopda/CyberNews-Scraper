# CyberNews-Scraper

Perfect â€” thanks for the clarification. Hereâ€™s the updated and final README.md, now with accurate dependency breakdowns based on script functionality (CSV vs PDF):

â¸»

ğŸ“° News Scraper

This tool scrapes headlines from Google News RSS feeds for any search keyword you enter. Outputs can be saved as a CSV or exported as a formatted PDF using Jinja2 and WeasyPrint.

Great for OSINT, threat intel snapshots, or research reporting â€” all from the command line.

â¸»

âœ… Features
	â€¢	ğŸ” Google News keyword search via RSS
	â€¢	ğŸ§  Extracts:
	â€¢	Title
	â€¢	URL
	â€¢	Source
	â€¢	Publish Time
	â€¢	ğŸ—ƒï¸ Outputs:
	â€¢	Excel format (news_scraper_xl.py)
	â€¢	PDF format (news_scraper.py)
	â€¢	Minimal setup, easy to customize

â¸»

ğŸ“¦ Dependencies

Install all dependencies at once:

pip install requests beautifulsoup4 feedparser pandas jinja2 weasyprint

Breakdown by script:

Script	Dependencies
news_scraper_csv.py	requests, beautifulsoup4, feedparser, pandas, os
news_scraper.py	requests, beautifulsoup4, feedparser, jinja2, weasyprint, os

ğŸ’¡ Note: os is part of Pythonâ€™s standard library.

â¸»

ğŸš€ How to Use

Clone the repo

git clone https://github.com/joychopda/news-scraper.git
cd news-scraper

Excel Output

python news_scraper_xl.py

Creates cyber_news.xlsx in your current directory.

PDF Output

python news_scraper_pdf.py

Creates a styled cyber_news.pdf.

Both scripts will prompt you for a search keyword.

â¸»

ğŸ“„ Sample Output

Excel (cyber_news.xlsx):

Title	Link	Source	Published
Major Cyberattack Hits	https://news.example.com	Wired	2025-06-20T08:00:00Z

PDF (cyber_news.pdf):
Clean, readable format with articles sorted and styled via HTML/CSS (Jinja2 + WeasyPrint).

â¸»

ğŸ” Use Cases
	â€¢	Threat intel snapshotting
	â€¢	Daily news digests for cybersecurity teams
	â€¢	OSINT enrichment
	â€¢	Competitive monitoring
	â€¢	Report-ready PDF exports

â¸»

âš ï¸ Disclaimer
	â€¢	Google News RSS is unofficial and may rate-limit or block aggressive scraping.
	â€¢	For educational and non-commercial use. Always respect robots.txt and ToS.


